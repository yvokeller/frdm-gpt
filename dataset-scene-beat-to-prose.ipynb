{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scene Beat to Prose\n",
    "\n",
    "Train the LLM to write paragraphs in my writing style, based on scene beats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "238it [22:26,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene beats generation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def separate_chapters(text):\n",
    "    chapters = []\n",
    "    lines = text.split('\\n')\n",
    "    current_chapter = []\n",
    "    weekdays = ['Montag', 'Dienstag', 'Mittwoch', 'Donnerstag', 'Freitag', 'Samstag', 'Sonntag']\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if i < len(lines) - 1 and any(lines[i+1].startswith(day) for day in weekdays):\n",
    "            if current_chapter:\n",
    "                chapters.append('\\n'.join(current_chapter))\n",
    "                current_chapter = []\n",
    "        current_chapter.append(line)\n",
    "    \n",
    "    if current_chapter:\n",
    "        chapters.append('\\n'.join(current_chapter))\n",
    "    \n",
    "    return chapters\n",
    "\n",
    "def chunk_chapters(chapters, max_words=250):\n",
    "    chunked_chapters = []\n",
    "    for chapter in chapters:\n",
    "        lines = chapter.split('\\n')\n",
    "        current_chunk = []\n",
    "        word_count = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            line_words = len(line.split())\n",
    "            if word_count + line_words > max_words:\n",
    "                if current_chunk:\n",
    "                    chunked_chapters.append('\\n'.join(current_chunk))\n",
    "                current_chunk = [line]\n",
    "                word_count = line_words\n",
    "            else:\n",
    "                current_chunk.append(line)\n",
    "                word_count += line_words\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunked_chapters.append('\\n'.join(current_chunk))\n",
    "    \n",
    "    return chunked_chapters\n",
    "\n",
    "def paragraph_to_scene_beats(chunk, chunk_id, model, tokenizer):\n",
    "    prompt = f\"\"\"\n",
    "    Du bist ein professioneller Autor und arbeitest an einer Fantasy-Szene. Vor dir liegt ein Textparagraph, und deine Aufgabe ist es, die zugrundeliegenden Scene Beats zu rekonstruieren. Jeder Beat sollte die Motivationen der Charaktere, die äusseren Umstände und das Ziel der Szene zusammenfassen, aber auf einfache und knappe Weise.\n",
    "\n",
    "    Hier ist ein Beispiel für einen Scene Beat, wie du ihn schreiben sollst:\n",
    "\n",
    "    - Die Krieger bereiten sich auf die letzte Schlacht vor. Der Himmel ist düster und bedrohlich. Die Krieger sind angespannt, hören nur ihre eigene Ausrüstung und Atmung. Auf das Kommando ihres Königs stürmen sie entschlossen voran.\n",
    "\n",
    "    Der Paragraph:\n",
    "\n",
    "    \"{chunk}\"\n",
    "\n",
    "    Antworte nur mit den Scene Beat(s), ohne weitere Einleitung oder Erklärung.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        scene_beats = generate(model, tokenizer, prompt=prompt, verbose=False, temp=0.7, max_tokens=1000)     \n",
    "        return scene_beats\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating rephrased text: {e}\")\n",
    "        return f\"EXCEPTION: {e}\"\n",
    "\n",
    "# Function to generate scene beats for chunks\n",
    "def generate_scene_beats(input_file, output_file):\n",
    "    # Load existing results if any\n",
    "    existing_results = {}\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                existing_results[data['id']] = data\n",
    "\n",
    "    # Process chunks and generate scene beats\n",
    "    with open(input_file, 'r') as in_file, open(output_file, 'a') as out_file:\n",
    "        for line in tqdm(in_file):\n",
    "            data = json.loads(line)\n",
    "            chunk_id = data['id']\n",
    "            \n",
    "            # Skip if already processed\n",
    "            if chunk_id in existing_results:\n",
    "                continue\n",
    "            \n",
    "            scene_beats = paragraph_to_scene_beats(data['chunk'], chunk_id, model, tokenizer)\n",
    "            \n",
    "            result = {\n",
    "                \"id\": chunk_id,\n",
    "                \"chunk\": data['chunk'],\n",
    "                \"scene_beats\": scene_beats\n",
    "            }\n",
    "            \n",
    "            json.dump(result, out_file)\n",
    "            out_file.write('\\n')\n",
    "            out_file.flush()  # Ensure data is written immediately\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load(\"models/frdm-Llama-3.1-8B-Write\")\n",
    "\n",
    "# Read and process the text\n",
    "text = open('data/ansturm.txt', 'r').read()\n",
    "chapters = separate_chapters(text)\n",
    "chunked_chapters = chunk_chapters(chapters)\n",
    "\n",
    "# Save chunked chapters to a JSONL file\n",
    "chunked_file = 'data/prose_paragraphs.jsonl'\n",
    "with open(chunked_file, 'w') as f:\n",
    "    for i, chunk in enumerate(chunked_chapters):\n",
    "        json.dump({\"id\": i, \"chunk\": chunk}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "# Generate scene beats\n",
    "output_file = 'data/prose_paragraphs_with_beats.jsonl'\n",
    "generate_scene_beats(chunked_file, output_file)\n",
    "\n",
    "print(\"Scene beats generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset creation completed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def create_dataset_entry(scene_beats, paragraph):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are the fantasy author Yvo K. Each time I prompt you with a scene beat, write the full scene based on the idea. Do not conclude the scene on your own, follow the beat instructions closely. Do not end with foreshadowing.\"},\n",
    "        {\"role\": \"user\", \"content\": scene_beats},\n",
    "        {\"role\": \"assistant\", \"content\": paragraph}\n",
    "    ]\n",
    "    return json.dumps({\"messages\": messages})\n",
    "\n",
    "def create_dataset(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            dataset_entry = create_dataset_entry(data['scene_beats'], data['chunk'])\n",
    "            outfile.write(dataset_entry + '\\n')\n",
    "\n",
    "# Create the dataset\n",
    "input_file = 'data/prose_paragraphs_with_beats.jsonl'\n",
    "output_file = 'data/scene_beat_to_prose_dataset.jsonl'\n",
    "create_dataset(input_file, output_file)\n",
    "\n",
    "print(\"Dataset creation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-validation split created in data/frdm-Llama-3.1-8B-Write-Beat-to-Prose-v1\n",
      "Train samples: 190\n",
      "Validation samples: 48\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "def create_train_valid_split(input_file, output_folder, train_ratio=0.8, seed=42):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Read all lines from the input file\n",
    "    with open(input_file, 'r') as infile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "    # Shuffle the lines\n",
    "    random.seed(seed)\n",
    "    random.shuffle(lines)\n",
    "\n",
    "    # Calculate the split index\n",
    "    split_index = int(len(lines) * train_ratio)\n",
    "\n",
    "    # Split the data\n",
    "    train_data = lines[:split_index]\n",
    "    valid_data = lines[split_index:]\n",
    "\n",
    "    # Write train data\n",
    "    train_file = os.path.join(output_folder, 'train.jsonl')\n",
    "    with open(train_file, 'w') as outfile:\n",
    "        outfile.writelines(train_data)\n",
    "\n",
    "    # Write validation data\n",
    "    valid_file = os.path.join(output_folder, 'valid.jsonl')\n",
    "    with open(valid_file, 'w') as outfile:\n",
    "        outfile.writelines(valid_data)\n",
    "\n",
    "    print(f\"Train-validation split created in {output_folder}\")\n",
    "    print(f\"Train samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(valid_data)}\")\n",
    "\n",
    "# Usage\n",
    "dataset_name = \"frdm-Llama-3.1-8B-Write-Beat-to-Prose-v1\"\n",
    "\n",
    "input_file = 'data/scene_beat_to_prose_dataset.jsonl'\n",
    "output_folder = f'data/{dataset_name}'\n",
    "create_train_valid_split(input_file, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frdm-gpt-AIfW-Lx2-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
