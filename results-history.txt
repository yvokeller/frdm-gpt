 /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/bigram.py
step 0: train loss 4.9466, val loss 4.9548
step 300: train loss 2.7186, val loss 2.7173
step 600: train loss 2.4266, val loss 2.4246
step 900: train loss 2.3735, val loss 2.3704
step 1200: train loss 2.3561, val loss 2.3541
step 1500: train loss 2.3584, val loss 2.3435
step 1800: train loss 2.3449, val loss 2.3321
step 2100: train loss 2.3392, val loss 2.3305
step 2400: train loss 2.3419, val loss 2.3354
step 2700: train loss 2.3466, val loss 2.3257

¬ªKene isins Nacht gersierem ireichorung Ersschaunin wimllichrhenkomke wer Aun ste Vor? 
¬ªGr. d.
¬ªVor Ded waschdur dien orufeich minte plticht. der w√∂lter ohlastrsemm ur. min r ks apt grs. ditzuchaben Wien Was kolachisen gerden, We duch Korge hendat, Se d deserirfen htt s Veichen den. dengendeiet Ben√∂lun. vor ufredtenauchmweraufr d an dichie s st √§t Kn sc3Hachein Genier iigrdie unte autehteserufohan. T√ºbe h Schdininde Ditn, akichr ich Dinden, Sch herumerdellte ble Un emm Frge zwenerrifl√∂t w√∂n gen
‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/bigram.py
step 0: train loss 4.9466, val loss 4.9548
step 300: train loss 2.7186, val loss 2.7173
step 600: train loss 2.4266, val loss 2.4246
step 900: train loss 2.3735, val loss 2.3704
step 1200: train loss 2.3561, val loss 2.3541
step 1500: train loss 2.3584, val loss 2.3435
step 1800: train loss 2.3449, val loss 2.3321
step 2100: train loss 2.3392, val loss 2.3305
step 2400: train loss 2.3419, val loss 2.3354
step 2700: train loss 2.3466, val loss 2.3257

Tisppe marben wenebttzibim hr eum. un, haurahend Ef be zdindimeromen bin vo Legand. lchndeosteirh Zu. Wal se jenege ver, del Sier m steuf√ºrch Gen ick√∂ne Jar stent t biendr de Seleichitrt m irtwauhrwier zie Weichirschin nderagerhndadanker ein egen den dam vos Menesezw√ºhwk√∂nleufextam h as? it, Mern.
¬ªEit sor vemehenuerht ielstert ssk en√∂ldeies meind Kolberanglersteich√§serzugenn Insen stweieith enn wus urmmin ht√ºbei√üm benendigen. fachruck√∂rmppp it ir wGenerabemurt henis, mungster w√§s andine korh ve
‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/bigram.py
True
step 0: train loss 4.9466, val loss 4.9548
step 300: train loss 2.7186, val loss 2.7173
step 600: train loss 2.4266, val loss 2.4246
step 900: train loss 2.3735, val loss 2.3704
step 1200: train loss 2.3561, val loss 2.3541
step 1500: train loss 2.3584, val loss 2.3435
step 1800: train loss 2.3449, val loss 2.3321
step 2100: train loss 2.3392, val loss 2.3305
step 2400: train loss 2.3419, val loss 2.3354
step 2700: train loss 2.3466, val loss 2.3257

Tisppe marben wenebttzibim hr eum. un, haurahend Ef be zdindimeromen bin vo Legand. lchndeosteirh Zu. Wal se jenege ver, del Sier m steuf√ºrch Gen ick√∂ne Jar stent t biendr de Seleichitrt m irtwauhrwier zie Weichirschin nderagerhndadanker ein egen den dam vos Menesezw√ºhwk√∂nleufextam h as? it, Mern.
¬ªEit sor vemehenuerht ielstert ssk en√∂ldeies meind Kolberanglersteich√§serzugenn Insen stweieith enn wus urmmin ht√ºbei√üm benendigen. fachruck√∂rmppp it ir wGenerabemurt henis, mungster w√§s andine korh ve

‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/bigram.py
True
False
step 0: train loss 4.9466, val loss 4.9548
step 300: train loss 2.7186, val loss 2.7173
step 600: train loss 2.4266, val loss 2.4246
^CTraceback (most recent call last):
  File "/Users/yvokeller/Code/frdmauthor/bigram.py", line 130, in <module>
    optimizer.step()
  File "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/torch/optim/adamw.py", line 184, in step
    adamw(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/torch/optim/adamw.py", line 335, in adamw
    func(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/torch/optim/adamw.py", line 413, in _single_tensor_adamw
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt

‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/bigram.py
True
False
step 0: train loss 4.9466, val loss 4.9548
step 300: train loss 2.7186, val loss 2.7173
step 600: train loss 2.4266, val loss 2.4246
step 900: train loss 2.3735, val loss 2.3704
step 1200: train loss 2.3561, val loss 2.3541
step 1500: train loss 2.3584, val loss 2.3435
step 1800: train loss 2.3449, val loss 2.3321
step 2100: train loss 2.3392, val loss 2.3305
step 2400: train loss 2.3419, val loss 2.3354
step 2700: train loss 2.3466, val loss 2.3257

¬ªKene isins Nacht gersierem ireichorung Ersschaunin wimllichrhenkomke wer Aun ste Vor? 
¬ªGr. d.
¬ªVor Ded waschdur dien orufeich minte plticht. der w√∂lter ohlastrsemm ur. min r ks apt grs. ditzuchaben Wien Was kolachisen gerden, We duch Korge hendat, Se d deserirfen htt s Veichen den. dengendeiet Ben√∂lun. vor ufredtenauchmweraufr d an dichie s st √§t Kn sc3Hachein Genier iigrdie unte autehteserufohan. T√ºbe h Schdininde Ditn, akichr ich Dinden, Sch herumerdellte ble Un emm Frge zwenerrifl√∂t w√∂n gen
‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python
Python 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:26:08) [Clang 14.0.6 ] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> print('Using device:', device)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'device' is not defined
>>> exit()
‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/bigram.py
Using device: cpu
step 0: train loss 4.9466, val loss 4.9548
step 300: train loss 2.7186, val loss 2.7173
step 600: train loss 2.4266, val loss 2.4246
step 900: train loss 2.3735, val loss 2.3704
step 1200: train loss 2.3561, val loss 2.3541
step 1500: train loss 2.3584, val loss 2.3435
step 1800: train loss 2.3449, val loss 2.3321
step 2100: train loss 2.3392, val loss 2.3305
step 2400: train loss 2.3419, val loss 2.3354
step 2700: train loss 2.3466, val loss 2.3257

¬ªKene isins Nacht gersierem ireichorung Ersschaunin wimllichrhenkomke wer Aun ste Vor? 
¬ªGr. d.
¬ªVor Ded waschdur dien orufeich minte plticht. der w√∂lter ohlastrsemm ur. min r ks apt grs. ditzuchaben Wien Was kolachisen gerden, We duch Korge hendat, Se d deserirfen htt s Veichen den. dengendeiet Ben√∂lun. vor ufredtenauchmweraufr d an dichie s st √§t Kn sc3Hachein Genier iigrdie unte autehteserufohan. T√ºbe h Schdininde Ditn, akichr ich Dinden, Sch herumerdellte ble Un emm Frge zwenerrifl√∂t w√∂n gen
‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/bigram.py
Using device: mps
step 0: train loss 4.9466, val loss 4.9548
step 300: train loss 2.7186, val loss 2.7173
step 600: train loss 2.4266, val loss 2.4246
step 900: train loss 2.3735, val loss 2.3704
step 1200: train loss 2.3561, val loss 2.3541
step 1500: train loss 2.3584, val loss 2.3435
step 1800: train loss 2.3449, val loss 2.3321
step 2100: train loss 2.3392, val loss 2.3305
step 2400: train loss 2.3419, val loss 2.3354
step 2700: train loss 2.3466, val loss 2.3257

Tisppe marben wenebttzibim hr eum. un, haurahend Ef be zdindimeromen bin vo Legand. lchndeosteirh Zu. Wal se jenege ver, del Sier m steuf√ºrch Gen ick√∂ne Jar stent t biendr de Seleichitrt m irtwauhrwier zie Weichirschin nderagerhndadanker ein egen den dam vos Menesezw√ºhwk√∂nleufextam h as? it, Mern.
¬ªEit sor vemehenuerht ielstert ssk en√∂ldeies meind Kolberanglersteich√§serzugenn Insen stweieith enn wus urmmin ht√ºbei√üm benendigen. fachruck√∂rmppp it ir wGenerabemurt henis, mungster w√§s andine korh ve
‚ùØ git init
Initialized empty Git repository in /Users/yvokeller/Code/frdmauthor/.git/
‚ùØ git add .
‚ùØ git commit -m 'Initial commit'
[main (root-commit) a5b7938] Initial commit
 3 files changed, 1330 insertions(+)
 create mode 100644 ansturm.txt
 create mode 100644 bigram.py
 create mode 100644 frdm-gpt.ipynb
‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/v2.py
Using device: mps
step 0: train loss 4.4658, val loss 4.4562
step 300: train loss 2.3832, val loss 2.3582
step 600: train loss 2.3568, val loss 2.3560
step 900: train loss 2.3571, val loss 2.3512
step 1200: train loss 2.3532, val loss 2.3525
step 1500: train loss 2.3559, val loss 2.3497
step 1800: train loss 2.3557, val loss 2.3374
step 2100: train loss 2.3443, val loss 2.3462
step 2400: train loss 2.3508, val loss 2.3466
step 2700: train loss 2.3592, val loss 2.3457

¬ªJalte mah geneteh min bimier eut. un, hastahen K√∂f be zdindimeroren bin vo Legand. lchn, osteis. Zu. Walele jenen Lut We gamar Po, steuf√ºrch Gen ickinehJar stentet bien d de meleichitrt m irtwamireier miel, Geiteschin n zuchrohndadanker ein egen den mam vost fnesezldabk√∂nleufextavo. as? it, Mern. dalag or vemehen. Z
¬ªFausstert mik en√∂ger es meien Kolien Rennesteich√§serzugenn. asen stweieith Den wus urmmin ht√ºbei√üe benen Tanendesocu Can mppp it ir wehehanbemert henis, mingster w√§s an ben korhlsc
‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/gpt.py
Using device: mps
Traceback (most recent call last):
  File "/Users/yvokeller/Code/frdmauthor/gpt.py", line 163, in <module>
    losses = estimate_loss()
  File "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/Users/yvokeller/Code/frdmauthor/gpt.py", line 66, in estimate_loss
    logits, loss = model(X, Y)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/yvokeller/Code/frdmauthor/gpt.py", line 123, in forward
    x = self.sa_head(x) # apply one head of self-attention (B,T,C)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/yvokeller/Code/frdmauthor/gpt.py", line 96, in forward
    wei = self.dropout(wei)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1695, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'Head' object has no attribute 'dropout'
‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/gpt.py
Using device: mps
step 0: train loss 4.4242, val loss 4.4234
step 500: train loss 2.5317, val loss 2.5404
step 1000: train loss 2.3673, val loss 2.3624
step 1500: train loss 2.3211, val loss 2.3207
step 2000: train loss 2.3028, val loss 2.2880
step 2500: train loss 2.2816, val loss 2.2767
step 3000: train loss 2.2849, val loss 2.2730
step 3500: train loss 2.2737, val loss 2.2589
step 4000: train loss 2.2494, val loss 2.2496
step 4500: train loss 2.2497, val loss 2.2441



W√∂lt marben wenebttz bim Krweum. In, waurgech√∂ng vod zeindimeromen zwielo Lega√üe. Dabersoscher. Zuer Hiele jene uft r√ºs Lamte Po, dreuf√ºrch eherickine Jarvstent tgeir, dert ischen.¬´ Er daurn! Ihrich om Pl√§des irschin nderagerhn Pfanker ein eg de b√∂nlder Beh en zuz dabk√∂nleuf ind. Ich habt, Mern.
¬ªEist oh ew√ºrhen. Z

Fiel Sert sst ew√∂vor es Has djesind anglerstein. Ger Dachn hasten Gabeirth ezu wus urmmin ht√ºbei√üe ben nd Trechesoch C√∂armpeprit √§hne hehanbemert henist ming Der w√§s and an komm ve
‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/gpt.py/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/bigram-multi-head-self-attention.py
/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python: can't open file '/Users/yvokeller/Code/frdmauthor/gpt.py/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python': [Errno 2] No such file or directory
‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/bigram-multi-head-self-attention.py
Using device: mps
step 0: train loss 4.4562, val loss 4.4610
step 500: train loss 2.5005, val loss 2.5032
step 1000: train loss 2.3333, val loss 2.3261
step 1500: train loss 2.2656, val loss 2.2569
step 2000: train loss 2.2265, val loss 2.2046
step 2500: train loss 2.1877, val loss 2.1790
step 3000: train loss 2.1792, val loss 2.1613
step 3500: train loss 2.1553, val loss 2.1331
step 4000: train loss 2.1240, val loss 2.1165
step 4500: train loss 2.1168, val loss 2.1055

Tlalte mahnen wenhertz bim ur eut. 
Gvorlichhen K√∂ffbe zut. 
Neigond bister Keinke. 
Sanden. Ich da er Hielf dmach eit un gande Po, drauf ich eheigegire Jarversit t√§bieger der, lein.¬´ Epem irt! Ihrich lich unde irschen nie anzt gerdanket ein etin dam mam vond ens ezwedek√∂nleus ind. Ich miit, anrn.
¬ªVie Mohreckt eiverht imt Seit sis ew√∂lde es Hallde sind anglersteinen Sit genn. 
Sen staben Moch, daus urmmen habl an. Dues die und auf unn Sicht inder weheht berert henist mitgster w√§rstichan komhen 
‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/bigram-multi-head-self-attention.py
Using device: mps
step 0: train loss 4.4150, val loss 4.4130
step 500: train loss 2.4136, val loss 2.3948
step 1000: train loss 2.2538, val loss 2.2429
step 1500: train loss 2.1945, val loss 2.1814
step 2000: train loss 2.1473, val loss 2.1173
step 2500: train loss 2.0950, val loss 2.0809
step 3000: train loss 2.0728, val loss 2.0565
step 3500: train loss 2.0640, val loss 2.0319
step 4000: train loss 2.0322, val loss 2.0194
step 4500: train loss 2.0078, val loss 1.9877

Tlad. 
Gannuncuten mich im Krecht. 
Gegelicht dang von zu nich ausst zwir ohnegand. 
Sterso. ¬ªVer Zu. Wal seinmum stut seing de Po, dreuf ich sheigegine Jarver nert bieger der, um Beit mippirn derreit mit Rud sites dir nichmgerhen danken einmegen damm. Momteht nes zweifk√∂nleuf der. Ich habt, anrn der kontrutwerhen eihm im wir verst ew√∂vor es miede Kolber Rinner Wir vom Miten, dassen Gebenithen, dall unmmen habe an um Send Trech auf und richt in ihne hehr ber bege wieln wagster ges and an kommen 

‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/gpt-residual-connections.py
Using device: mps
step 0: train loss 4.4508, val loss 4.4463
step 500: train loss 2.3100, val loss 2.2797
step 1000: train loss 2.1522, val loss 2.1207
step 1500: train loss 2.0831, val loss 2.0477
step 2000: train loss 2.0032, val loss 1.9838
step 2500: train loss 1.9822, val loss 1.9494
step 3000: train loss 1.9513, val loss 1.9103
step 3500: train loss 1.8906, val loss 1.8858
step 4000: train loss 1.8778, val loss 1.8576
step 4500: train loss 1.8654, val loss 1.8435

Tlat.
¬ªGarben wenn mich im Krecht. In, haben mein freutegenig verund zwiel Tarz kein kannso. Ich da er Hauff dem vervorcu gange Porcortuf ich ehen Walde? Nehr entft bider watz, leibeit vernige! Ihren. ¬ªWie er sit schinde zumgerhallen.

Moptige. Ich mit mich hich zuz dabk√∂nleungleich. Ich ist anrnef√§llt sorgewegereuerhelt gringt! Wenen. Meine Hallt stlich Renner Wald sein genichtsen starzeithen, dass unmerauft√ºr an. Dusk die Ocher Punch begepritt√§fte hehteben begren sind, grieser, sondene kommene

‚ùØ /opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/bin/python /Users/yvokeller/Code/frdmauthor/gpt-2-layernorm.py
Using device: mps
step 0: train loss 4.5611, val loss 4.5529
step 500: train loss 2.2694, val loss 2.2415
step 1000: train loss 2.0935, val loss 2.0635
step 1500: train loss 2.0177, val loss 1.9787
step 2000: train loss 1.9483, val loss 1.9304
step 2500: train loss 1.9163, val loss 1.8884
step 3000: train loss 1.8881, val loss 1.8542
step 3500: train loss 1.8354, val loss 1.8272
step 4000: train loss 1.8211, val loss 1.8107
step 4500: train loss 1.8166, val loss 1.7952

Tlat.
¬ªGarzum Abendren bin vorcum. Der Mir gemetal von zum Wimmer entzimmer angand. Dabeiso. ¬ªGrupor. Wie leunmur und von einer Portrtet hich von ich wei√ü das entfogen geradt melten.¬´ Er mainnetringt. ¬ªWie um die M√§ruf nie anzehen danken. Wir genauben. Moment entzez dabk√∂nnen. Werw√∂. Is? ist Mern√∂fplikt ohnen

Einversten Amurett stalnende Ast, sein Dieso RRangen.

SMALL:

¬ª¬ªUm Miten, hasten vor einem zu wurdurch. Ihr du hat zu nach und auf und begeprunderne Mehreben bege wieder, gatGrog, mand vor. Er ve


Sitz besorhen sim enten wir, ein uns horeckten mir.wor Facht. 
¬ªIch beerhabstter an ich klat l√∂ffehren gewirze. Ob. √Ñnlen. Minen in Mil hat tramme weittigen Tragersta milen sicht simm Werwarinds hau hanzt vierschein wir dunder Schliss enweich einen sil, du den Zeuern zur√ºck? Ich so misst die Keren zu leich ersteken aus im Sear nichme mit lie pahr und erst ist meinschlicht vers√§ndend nad stitem Tilt beif m√ºr ich nach sar erepe Zur dass√ºrter Aute beh√§nder um hertell, ich nicht diese Schin gas eenh


 python train_model.py
['\n', ' ', '!', ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '¬´', '¬ª', '√Ñ', '√ñ', '√ú', '√ü', '√§', '√∂', '√º', '‚Äì', '‚Äô', '‚Äö']
Total parameters: 10801233
Trainable parameters: 10801233
wandb: Currently logged in as: yvokeller (sensor-based-activity-recognition). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: yvokeller. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /Users/yvokeller/Code/frdm-gpt/src/wandb/run-20231022_235404-epq7y3gn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-firefly-12
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yvokeller/frdm-GPT
wandb: üöÄ View run at https://wandb.ai/yvokeller/frdm-GPT/runs/epq7y3gn
step 0: train loss 4.5630, val loss 4.5612
step 250: train loss 2.2677, val loss 2.2598
step 500: train loss 1.7569, val loss 1.7465
step 750: train loss 1.4854, val loss 1.5003
step 1000: train loss 1.3292, val loss 1.3864
step 1250: train loss 1.2156, val loss 1.3251
step 1500: train loss 1.1201, val loss 1.2900
step 1750: train loss 1.0356, val loss 1.2698
step 2000: train loss 0.9505, val loss 1.2646
step 2250: train loss 0.8663, val loss 1.2781
step 2500: train loss 0.7886, val loss 1.3033
step 2750: train loss 0.7102, val loss 1.3408
step 3000: train loss 0.6240, val loss 1.3715
step 3250: train loss 0.5509, val loss 1.4135
step 3500: train loss 0.4814, val loss 1.4705
step 3750: train loss 0.4121, val loss 1.5394
step 4000: train loss 0.3537, val loss 1.5826
step 4250: train loss 0.2983, val loss 1.6663
step 4500: train loss 0.2502, val loss 1.7238
step 4750: train loss 0.2136, val loss 1.7947
step 4999: train loss 0.1800, val loss 1.8650

W√ºrden und der Gestalt werden, versuch immer mehr w√§ren. Die √Ñuhe verblickt kann ich die Einf√ºhrung der Menschheit ineingelangen Lauft von seiner Stadt betrachten, um die Ordne zwischen zwei Tung liegen muss. Was mir diese zerfliegen deiner ein.¬´ 
Endg√ºlt ert√∂nen Tag reicht. Eine hdere Stimme in tiefer tot und entfernt sind die letzte Leben und verschwundende Gruppe wird vom. Der Begegenbegie, die √úberpr√ºckwocher auf die anderen, habe ich mir vorhin die Zukunft. Eine Maume ist gibt Schn sowein j
wandb: Waiting for W&B process to finish... (success).