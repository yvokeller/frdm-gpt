{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sokrate  Flucht\n",
      "Montag, 22. September 2014\n",
      "\n",
      "Ich sehe die Nacht hereinbrechen. Dunkelheit umhüllt nach und nach die karge Landschaft, mir wird zunehmend kälter. Ich muss mich beeilen, mir bleibt nicht mehr viel Zeit. In wenigen Minuten wird es zu spät sein, alles vorbei, mein Leben sich in Luft auflösen, meine Erinnerungen sich verflüchtigen, mein Ich im Nichts verschwinden. Was habe ich mir nur dabei gedacht. Dass ich jetzt hier stehe, an diesem Ort, von dem niemand etwas ahnt, geschweige denn weiß, habe ich wieder einzig meinem sturen Kopf zu verdanken.\n",
      "Ich erreiche den Wald, mein Herz schlägt höher. Was werden die Folgen von meinem unerwünschten Eindringen hier sein? Schreckliche Gedanken gehen mir durch den Kopf. Aber es ist die Gefahr wert, oder? Ich habe viele wichtige Informationen bekommen, welche die Menschheit vor einer gewaltigen Bedrohung schützen könnten. Vorausgesetzt, es kommt jemals dazu, dass sie einen Menschen erreichen. Und die Chance, dass der mir diese unvorstellbar\n"
     ]
    }
   ],
   "source": [
    "with open('ansturm.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "    punctuation = {\n",
    "        u'\\u2003': \" \",\n",
    "    }\n",
    "    \n",
    "    for key, value in punctuation.items():\n",
    "        text = text.replace(key, value)\n",
    "\n",
    "# A sample of the text\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 295869\n",
      "Number of words: 46581\n",
      "Vocabulary size: 81\n",
      "['\\n', ' ', '!', ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '«', '»', 'Ä', 'Ö', 'Ü', 'ß', 'ä', 'ö', 'ü', '–', '’', '‚']\n"
     ]
    }
   ],
   "source": [
    "# EDA on the text\n",
    "print('Number of characters:', len(text))\n",
    "print('Number of words:', len(text.split(' ')))\n",
    "\n",
    "# Unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [24, 42, 53, 53, 56, 1, 39, 46, 53, 61, 2]\n",
      "Decoded: Hallo Welt!\n"
     ]
    }
   ],
   "source": [
    "# Tokenization (simple character-level encoding)\n",
    "stoi = { ch: i for i, ch in enumerate(chars) }\n",
    "itos = { i: ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print('Encoded:', encode('Hallo Welt!'))\n",
    "print('Decoded:', decode(encode('Hallo Welt!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([295869]) torch.int64\n",
      "tensor([35, 56, 52, 59, 42, 61, 46,  1,  1, 22, 53, 62, 44, 49, 61,  0, 29, 56,\n",
      "        55, 61, 42, 48,  3,  1,  7,  7,  4,  1, 35, 46, 57, 61, 46, 54, 43, 46,\n",
      "        59,  1,  7,  5,  6,  9,  0,  0, 25, 44, 49,  1, 60, 46, 49, 46,  1, 45,\n",
      "        50, 46,  1, 30, 42, 44, 49, 61,  1, 49, 46, 59, 46, 50, 55, 43, 59, 46,\n",
      "        44, 49, 46, 55,  4,  1, 20, 62, 55, 52, 46, 53, 49, 46, 50, 61,  1, 62,\n",
      "        54, 49, 77, 53, 53, 61,  1, 55, 42, 44, 49,  1, 62, 55, 45,  1, 55, 42,\n",
      "        44, 49,  1, 45, 50, 46,  1, 52, 42, 59, 48, 46,  1, 28, 42, 55, 45, 60,\n",
      "        44, 49, 42, 47, 61,  3,  1, 54, 50, 59,  1, 64, 50, 59, 45,  1, 67, 62,\n",
      "        55, 46, 49, 54, 46, 55, 45,  1, 52, 75, 53, 61, 46, 59,  4,  1, 25, 44,\n",
      "        49,  1, 54, 62, 60, 60,  1, 54, 50, 44, 49,  1, 43, 46, 46, 50, 53, 46,\n",
      "        55,  3,  1, 54, 50, 59,  1, 43, 53, 46, 50, 43, 61,  1, 55, 50, 44, 49,\n",
      "        61,  1, 54, 46, 49, 59,  1, 63, 50, 46, 53,  1, 41, 46, 50, 61,  4,  1,\n",
      "        25, 55,  1, 64, 46, 55, 50, 48, 46, 55,  1, 29, 50, 55, 62, 61, 46, 55,\n",
      "         1, 64, 50, 59, 45,  1, 46, 60,  1, 67, 62,  1, 60, 57, 75, 61])\n"
     ]
    }
   ],
   "source": [
    "# Encode the text into data tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader with Batch and Time Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test split\n",
    "n = int(0.8 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block / Context size\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When context: tensor([35]), -> target: 56\n",
      "When context: tensor([35, 56]), -> target: 52\n",
      "When context: tensor([35, 56, 52]), -> target: 59\n",
      "When context: tensor([35, 56, 52, 59]), -> target: 42\n",
      "When context: tensor([35, 56, 52, 59, 42]), -> target: 61\n",
      "When context: tensor([35, 56, 52, 59, 42, 61]), -> target: 46\n",
      "When context: tensor([35, 56, 52, 59, 42, 61, 46]), -> target: 1\n",
      "When context: tensor([35, 56, 52, 59, 42, 61, 46,  1]), -> target: 1\n"
     ]
    }
   ],
   "source": [
    "# Time dimension\n",
    "# Note: Transformer will be able to predict from context as short as a single character, up to the whole block size\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When context: {context}, -> target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We get 32 examples with a single batch , each of them completely independent as far as the transformer is concerned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 46, 55,  1, 20, 42, 55, 52],\n",
      "        [46, 59, 60, 50, 57,  1, 64, 50],\n",
      "        [46, 59,  1, 39, 42, 53, 45,  4],\n",
      "        [56, 44, 49,  1, 46, 60,  1, 64]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[46, 55,  1, 20, 42, 55, 52,  1],\n",
      "        [59, 60, 50, 57,  1, 64, 50, 59],\n",
      "        [59,  1, 39, 42, 53, 45,  4,  1],\n",
      "        [44, 49,  1, 46, 60,  1, 64, 42]])\n",
      "Batch 0:\n",
      "When context: tensor([53]), -> target: 46\n",
      "When context: tensor([53, 46]), -> target: 55\n",
      "When context: tensor([53, 46, 55]), -> target: 1\n",
      "When context: tensor([53, 46, 55,  1]), -> target: 20\n",
      "When context: tensor([53, 46, 55,  1, 20]), -> target: 42\n",
      "When context: tensor([53, 46, 55,  1, 20, 42]), -> target: 55\n",
      "When context: tensor([53, 46, 55,  1, 20, 42, 55]), -> target: 52\n",
      "When context: tensor([53, 46, 55,  1, 20, 42, 55, 52]), -> target: 1\n",
      "Batch 1:\n",
      "When context: tensor([46]), -> target: 59\n",
      "When context: tensor([46, 59]), -> target: 60\n",
      "When context: tensor([46, 59, 60]), -> target: 50\n",
      "When context: tensor([46, 59, 60, 50]), -> target: 57\n",
      "When context: tensor([46, 59, 60, 50, 57]), -> target: 1\n",
      "When context: tensor([46, 59, 60, 50, 57,  1]), -> target: 64\n",
      "When context: tensor([46, 59, 60, 50, 57,  1, 64]), -> target: 50\n",
      "When context: tensor([46, 59, 60, 50, 57,  1, 64, 50]), -> target: 59\n",
      "Batch 2:\n",
      "When context: tensor([46]), -> target: 59\n",
      "When context: tensor([46, 59]), -> target: 1\n",
      "When context: tensor([46, 59,  1]), -> target: 39\n",
      "When context: tensor([46, 59,  1, 39]), -> target: 42\n",
      "When context: tensor([46, 59,  1, 39, 42]), -> target: 53\n",
      "When context: tensor([46, 59,  1, 39, 42, 53]), -> target: 45\n",
      "When context: tensor([46, 59,  1, 39, 42, 53, 45]), -> target: 4\n",
      "When context: tensor([46, 59,  1, 39, 42, 53, 45,  4]), -> target: 1\n",
      "Batch 3:\n",
      "When context: tensor([56]), -> target: 44\n",
      "When context: tensor([56, 44]), -> target: 49\n",
      "When context: tensor([56, 44, 49]), -> target: 1\n",
      "When context: tensor([56, 44, 49,  1]), -> target: 46\n",
      "When context: tensor([56, 44, 49,  1, 46]), -> target: 60\n",
      "When context: tensor([56, 44, 49,  1, 46, 60]), -> target: 1\n",
      "When context: tensor([56, 44, 49,  1, 46, 60,  1]), -> target: 64\n",
      "When context: tensor([56, 44, 49,  1, 46, 60,  1, 64]), -> target: 42\n"
     ]
    }
   ],
   "source": [
    "# Batch & time dimension\n",
    "torch.manual_seed(42)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs × and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # get batch_size # of random integer offsets within the data\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack the tensors of context slices\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # stack the tensors of predicted slices, offset by +1 from context\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print (yb.shape)\n",
    "print (yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    print(f'Batch {b}:')\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'When context: {context}, -> target: {target}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 81])\n",
      "tensor(4.9485, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "gvIVlTdM9WD»F7äüAVzpk4eDmYybÜQ«o.,zEhS‚Y43AdfZDm Lßf–R»!GHm«R’nq HqmQpoqBm’p8rGH«|9Rxc‚MA0»äJNGC9wu2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # logits are basically the scores for the next character in the sequence\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) dimension where C is the vocab size\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # need to reshape logits as expected by F.cross_entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # stretch out the first two dimensions, keep channel C as 2nd dimension\n",
    "            targets = targets.view(B*T) # make one dimensional\n",
    "            loss = F.cross_entropy(logits, targets) # also called \"negative log-likelihood\" loss\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# Generate some text on untrained model\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4390273094177246\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000): # increase number of steps for good results... \n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train') # get batch of data\n",
    "\n",
    "    # forward pass\n",
    "    logits, loss = m(xb, yb) # evaluate the loss\n",
    "\n",
    "    # clear the gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "\n",
    "    # get gradients for all parameters and update parameters accordingly\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Re ert siaurens ust wase miert ewaräch benwin agtanen gl st.\n",
      "intteugeund Schau d brt hererchneleinen\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical trick  for self-attention\n",
    "\n",
    "#### Version 1: Weighted aggregation of past context with for loops\n",
    "\n",
    "We want $x[b,t] = mean_{i<=t} x[b,i]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch size, time steps, channels\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C)) # Bag-of-Words representation\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] # the 0th batch element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0] # e.g the the second row is now an average of the 0th, 1st and 2nd rows of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above is **really** inefficient, so we want to use matrix operations instead:\n",
    "\n",
    "#### Version 2: Weighted aggregation for past context with matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones (3, 3)) # lower triangular matrix\n",
    "a = a / torch.sum(a, 1, keepdim=True) # normalize rows to sum to 1 (helps us average!\n",
    "b = torch.randint(0, 10, (3,2)).float() \n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "weights = torch.tril(torch.ones(T, T)) # again, our triangular matrix\n",
    "weights = weights / weights.sum(1, keepdim=True) # normalize rows to sum to 1\n",
    "\n",
    "# batched matrix multiply\n",
    "xbow2 = weights @ x # (B, T, T) @ (B, T, C) ----> (B, T, C) | means: (T, T) @ (T,C) for each batch element, resulting in (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 3: Weighted aggregation for past context with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "lower_triangular = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros((T,T)) # 0s bow, but can be viewed as \"interaction strength\" between tokens! (how interesting they find each other)\n",
    "weights = weights.masked_fill(lower_triangular == 0, float('-inf')) # tokens from the past cannot interact with tokens from the future! therefore, mask out the upper triangular part with -inf \n",
    "weights = F.softmax(weights, dim=-1) # softmax over all rows\n",
    "xbow3 = weights @ x # aggregate the token's values depending on their interaction strength \n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 4: Self-attention!\n",
    "\n",
    "Instead of simply averaging, instead, we want \"interaction strength\" to be data-dependent, not just an average!\n",
    "\n",
    "How self-attention solves this:\n",
    "- Every single token will emit two vectors: a query vector (what am I looking for?) and a key vector (what do I contain?)\n",
    "- We calculate the dot product between the query vector of our given token and the key vector of every other token, yielding a vector of scores. Thus, if key and query vectors are \"aligned\", they will interact with a high amount (the dot product will yield a higher number). This leads to attention on the tokens that are most relevant to each other. This is the core of self-attention.\n",
    "- There is a third vector, value (if you find me interesting, here's what I have to communicate to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4,8,32 # batch, time, channels | note: we now use 32 channels instead of 2, meaning we have 32 features per token\n",
    "x = torch.randn (B, T, C)\n",
    "\n",
    "lower_triangular = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros((T,T)) # instead, we want \"interaction strength\" to be data-dependent, not just an average!\n",
    "weights = weights.masked_fill(lower_triangular == 0, float('-inf'))\n",
    "weights = F.softmax (weights, dim=-1)\n",
    "out = weights @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels | note: we now use 32 channels instead of 2, meaning we have 32 features per token\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "# for every row of B we now get a matrix of size (T, T) giving us the interaction strength between tokens!\n",
    "weights = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T) | note: we only transpose the last two dimensions, not the batch dimension!\n",
    "\n",
    "lower_triangular = torch.tril(torch.ones(T, T))\n",
    "#weights = torch.zeros((T,T))\n",
    "weights = weights.masked_fill(lower_triangular == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "#weights = weights @ x\n",
    "out = weights @ v # perform the weighted aggregation of the values\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the last row of our first (T, T) matrix, we for example see what the 8th token has a high attention towards. For example, to the 4th token:\n",
    "\n",
    "[0.0210, 0.0843, 0.0555, 4th: **_0.2297_**, 0.0573, 0.0709, 0.2423, 8th: **0.2391**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now every single batch element has a different interaction strength matrix! \n",
    "weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. For an autoregressive language model, this can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "  \n",
    "<img src=\"assets/attention-graph-viz.svg\" alt=\"attention-graph-viz\" width=\"400\"/>\n",
    "\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an **\"encoder\"** attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `weights` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, weights will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrating the effect of scaled attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "weights_raw = q @ k.transpose(-2, -1)\n",
    "weights_scaled = weights_raw / math.sqrt(head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k var tensor(1.0449)\n",
      "q var tensor(1.0700)\n",
      "weights_raw var tensor(17.4690)\n",
      "weights_scaled var tensor(1.0918)\n"
     ]
    }
   ],
   "source": [
    "# print variances for each\n",
    "print('k var', k.var())\n",
    "print('q var', q.var())\n",
    "print('weights_raw var', weights_raw.var())\n",
    "print('weights_scaled var', weights_scaled.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo with example data\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # without scaling and big value differences, softmax gets too peaky focusing on max, and converges to one-hot encoded vectors\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
